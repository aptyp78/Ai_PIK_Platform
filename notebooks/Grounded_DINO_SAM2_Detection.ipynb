{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d596de28",
   "metadata": {
    "id": "d596de28"
   },
   "source": [
    "# GroundedDINO + SAM — Detection (Colab Pro+) — commit 3767bc9\n",
    "Детектор регионов: монтируем GCS (через сервис‑аккаунт), ставим Torch+детекторы, рендерим страницы, запускаем детекцию и грузим регионы в `gs://pik-artifacts-dev/grounded_regions/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run Control and Parameters\n",
    "# SIGPIPE-friendly stdout (avoid BrokenPipeError in Colab pipes)\n",
    "import signal\n",
    "if hasattr(signal, 'SIGPIPE'):\n",
    "    signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n",
    "\n",
    "# Toggle to start the pipeline\n",
    "START_RUN = False  #@param {type:\"boolean\"}\n",
    "\n",
    "# Key parameters (leave pages empty to process ALL pages)\n",
    "PLAYBOOK_PDF = '/content/src_gcs/playbooks/PIK - Expert Guide - Platform IT Architecture - Playbook - v11.pdf'  #@param {type:\"string\"}\n",
    "PAGES = []  #@param {type:\"raw\"}\n",
    "FRAME_NAMES_INPUT = 'PIK - Platform IT Architecture Canvas - Table View - v01.png, PIK - Platform IT Architecture Canvases - v01.png, PIK - Expert Guide - Platform IT Architecture - Assessment - v01.png'  #@param {type:\"string\"}\n",
    "PROMPTS_INPUT = 'diagram,canvas,table,legend,arrow,node'  #@param {type:\"string\"}\n",
    "BOX_THRESHOLD = 0.35  #@param {type:\"number\"}\n",
    "TEXT_THRESHOLD = 0.25  #@param {type:\"number\"}\n",
    "TOPK = 12  #@param {type:\"integer\"}\n",
    "DEVICE = 'auto'  #@param [\"auto\", \"cuda\", \"cpu\"]\n",
    "USE_SAM2 = True  #@param {type:\"boolean\"}\n",
    "\n",
    "REPORT_TO_GCS = True  #@param {type:\"boolean\"}\n",
    "GCS_BUCKET = 'pik-artifacts-dev'  #@param {type:\"string\"}\n",
    "RUN_TAG = ''  #@param {type:\"string\"}\n",
    "\n",
    "# Derived lists from string inputs\n",
    "FRAME_NAMES = [x.strip() for x in FRAME_NAMES_INPUT.split(',') if x.strip()]\n",
    "PROMPTS = [x.strip() for x in PROMPTS_INPUT.split(',') if x.strip()]\n",
    "OUT_PAGES_DIR = '/content/pages'\n",
    "DETECT_OUT = '/content/grounded_regions'\n",
    "\n",
    "# Helper to gate execution in subsequent cells\n",
    "def require_start():\n",
    "    if not START_RUN:\n",
    "        raise SystemExit('Execution gated. Set START_RUN=True in the top cell and rerun.')\n",
    "\n",
    "print('Configured. START_RUN=', START_RUN)\n",
    "print('PDF:', PLAYBOOK_PDF)\n",
    "print('PAGES (empty=ALL):', PAGES)\n",
    "print('Frames:', FRAME_NAMES)\n",
    "print('Prompts:', PROMPTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76900c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Cell Execution Logger\n",
    "import os, sys, json, time, uuid, warnings\n",
    "from pathlib import Path\n",
    "try:\n",
    "  LOG_DIR  # noqa: F821\n",
    "except NameError:\n",
    "  RUN_ID = time.strftime('%Y%m%d-%H%M%S')\n",
    "  LOCAL_LOG_ROOT = '/content/colab_runs'\n",
    "  LOG_DIR = Path(LOCAL_LOG_ROOT)/RUN_ID\n",
    "  LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from IPython import get_ipython\n",
    "ip = get_ipython()\n",
    "\n",
    "class _Tee:\n",
    "  def __init__(self, stream, buf_list):\n",
    "    self._s = stream; self._b = buf_list\n",
    "  def write(self, s):\n",
    "    try: self._s.write(s)\n",
    "    finally: self._b.append(s)\n",
    "  def flush(self):\n",
    "    try: self._s.flush()\n",
    "    except Exception: pass\n",
    "\n",
    "_celllog = {'i': None, 'start': None, 'buf_out':[], 'buf_err':[], 'warns':[], 'id': None}\n",
    "_orig_out, _orig_err = sys.stdout, sys.stderr\n",
    "_orig_showwarning = warnings.showwarning\n",
    "LOG_JSONL = str(LOG_DIR/'cells.jsonl')\n",
    "\n",
    "def _pre(cell_id):\n",
    "  _celllog['i'] = ip.execution_count + 1\n",
    "  _celllog['id'] = str(uuid.uuid4())\n",
    "  _celllog['start'] = time.time()\n",
    "  _celllog['buf_out'] = []\n",
    "  _celllog['buf_err'] = []\n",
    "  _celllog['warns'] = []\n",
    "  sys.stdout = _Tee(_orig_out, _celllog['buf_out'])\n",
    "  sys.stderr = _Tee(_orig_err, _celllog['buf_err'])\n",
    "  def _sw(message, category, filename, lineno, file=None, line=None):\n",
    "    _celllog['warns'].append({'message': str(message), 'category': getattr(category,'__name__', str(category)), 'filename': filename, 'lineno': lineno})\n",
    "    return _orig_showwarning(message, category, filename, lineno, file, line)\n",
    "  warnings.showwarning = _sw\n",
    "\n",
    "def _post(result):\n",
    "  # restore\n",
    "  sys.stdout = _orig_out\n",
    "  sys.stderr = _orig_err\n",
    "  warnings.showwarning = _orig_showwarning\n",
    "  end = time.time()\n",
    "  i = _celllog.get('i')\n",
    "  # Try to get cell source from history\n",
    "  src = None\n",
    "  try:\n",
    "    ih = ip.user_ns.get('_ih', [])\n",
    "    if i is not None and i < len(ih):\n",
    "      src = ih[i]\n",
    "  except Exception:\n",
    "    src = None\n",
    "  rec = {\n",
    "    'cell_id': _celllog.get('id'),\n",
    "    'execution_count': i,\n",
    "    'start_ts': _celllog.get('start'),\n",
    "    'end_ts': end,\n",
    "    'duration_s': (end - _celllog['start']) if _celllog.get('start') else None,\n",
    "    'success': bool(getattr(result, 'success', True)),\n",
    "    'out': ''.join(_celllog.get('buf_out') or []),\n",
    "    'err': ''.join(_celllog.get('buf_err') or []),\n",
    "    'warnings': _celllog.get('warns') or [],\n",
    "    'source': src,\n",
    "  }\n",
    "  try:\n",
    "    with open(LOG_JSONL, 'a', encoding='utf-8') as f:\n",
    "      f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "  except Exception as e:\n",
    "    print('[cell-logger] write failed:', e)\n",
    "\n",
    "ip.events.register('pre_run_cell', _pre)\n",
    "ip.events.register('post_run_cell', _post)\n",
    "print('[cell-logger] enabled ->', LOG_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91d7fd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d91d7fd2",
    "outputId": "9bea73c3-9cbe-46f6-dbfd-714a34b19fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook version: ef4284f\n",
      "Tue Sep 16 04:21:37 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "#@title Runtime & GPU\n",
    "NOTEBOOK_VERSION = '3767bc9'\n",
    "print('Notebook version:', NOTEBOOK_VERSION)\n",
    "# Runtime & GPU\n",
    "!nvidia-smi || true\n",
    "import sys; print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd390ea",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfd390ea",
    "outputId": "3ec50ffd-8d86-4430-ba72-aeb2946c4436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auth] Colab user credentials OK\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:8 https://packages.cloud.google.com/apt gcsfuse-jammy InRelease [1,227 B]\n",
      "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:12 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
      "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,441 kB]\n",
      "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,311 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,624 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [80.3 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [88.8 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,627 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
      "Get:23 https://packages.cloud.google.com/apt gcsfuse-jammy/main all Packages [750 B]\n",
      "Get:24 https://packages.cloud.google.com/apt gcsfuse-jammy/main amd64 Packages [45.2 kB]\n",
      "Get:25 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [43.2 kB]\n",
      "Get:26 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,795 kB]\n",
      "Get:27 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,267 kB]\n",
      "Fetched 35.6 MB in 3s (13.1 MB/s)\n",
      "Reading package lists...\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  gcsfuse poppler-utils\n",
      "0 upgraded, 2 newly installed, 0 to remove and 50 not upgraded.\n",
      "Need to get 15.3 MB of archives.\n",
      "After this operation, 697 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.10 [186 kB]\n",
      "Get:2 https://packages.cloud.google.com/apt gcsfuse-jammy/main amd64 gcsfuse amd64 3.3.0 [15.1 MB]\n",
      "Fetched 15.3 MB in 0s (47.6 MB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package poppler-utils.\n",
      "(Reading database ... 126374 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.10_amd64.deb ...\n",
      "Unpacking poppler-utils (22.02.0-2ubuntu0.10) ...\n",
      "Selecting previously unselected package gcsfuse.\n",
      "Preparing to unpack .../gcsfuse_3.3.0_amd64.deb ...\n",
      "Unpacking gcsfuse (3.3.0) ...\n",
      "Setting up gcsfuse (3.3.0) ...\n",
      "Setting up poppler-utils (22.02.0-2ubuntu0.10) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "[auth] SA key written to /content/Secrets/sa.json (source: GCS_SA_JSON )\n",
      "[auth] SA prepared. Proceed to mount in the next cell.\n"
     ]
    }
   ],
   "source": [
    "#@title Auth + gcsfuse setup\n",
    "require_start()\n",
    "\n",
    "# Install packages and prepare gcsfuse repo; auto-mount with SA from /content/Secrets if present\n",
    "# Try Colab user auth (optional)\n",
    "try:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print('[auth] Colab user credentials OK')\n",
    "except Exception as e:\n",
    "  print('[auth] Skipping Colab user auth:', e)\n",
    "!pip -q install google-cloud-storage gcsfs==2025.3.0 fsspec==2025.3.0\n",
    "!sudo install -m 0755 -d /usr/share/keyrings\n",
    "!curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg -o /tmp/cloud.google.gpg\n",
    "!sudo gpg --dearmor --yes --batch -o /usr/share/keyrings/cloud.google.gpg /tmp/cloud.google.gpg || sudo cp /tmp/cloud.google.gpg /usr/share/keyrings/cloud.google.gpg\n",
    "!echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt gcsfuse-jammy main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list >/dev/null\n",
    "!sudo apt-get -q update\n",
    "!sudo apt-get -q install -y gcsfuse poppler-utils\n",
    "!mkdir -p /content/src_gcs /content/artifacts /content/pages /content/Secrets\n",
    "import glob, os, subprocess, shlex\n",
    "\n",
    "# Read SA key from Colab Secrets (GCS_SA_JSON / GCS_SA_JSON2 / secretName) and optional GOOGLE_API_KEY\n",
    "try:\n",
    "  from google.colab import userdata as _ud\n",
    "  _sa = None; _sa_name = ''\n",
    "  for _k in ('GCS_SA_JSON','GCS_SA_JSON2','secretName'):\n",
    "    try:\n",
    "      _v = _ud.get(_k)\n",
    "    except Exception:\n",
    "      _v = None\n",
    "    if _v:\n",
    "      _sa = _v; _sa_name = _k; break\n",
    "  try:\n",
    "    _gapi = _ud.get('GOOGLE_API_KEY')\n",
    "  except Exception:\n",
    "    _gapi = None\n",
    "except Exception:\n",
    "  _sa = None; _gapi = None; _sa_name = ''\n",
    "if _gapi:\n",
    "  os.environ['GOOGLE_API_KEY'] = _gapi\n",
    "  print('[auth] GOOGLE_API_KEY loaded from Colab Secrets')\n",
    "if _sa:\n",
    "  os.makedirs('/content/Secrets', exist_ok=True)\n",
    "  _key_path = '/content/Secrets/sa.json'\n",
    "  open(_key_path,'w',encoding='utf-8').write(_sa)\n",
    "  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = _key_path\n",
    "  print('[auth] SA key written to', _key_path, '(source:', _sa_name, ')')\n",
    "\n",
    "# Skip auto-mount here; use the next cell 'Mount GCS buckets'\n",
    "print('[auth] SA prepared. Proceed to mount in the next cell.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Compatibility Fixes (pip pins)\n",
    "require_start()\n",
    "# Do not upgrade ipython (Colab expects ipython==7.34.0)\n",
    "# Remove xformers if present (version coupling to torch; optional)\n",
    "!pip -q uninstall -y xformers || true\n",
    "\n",
    "# Pin compatible versions for known conflicts\n",
    "!pip -q install -U   \"typing_extensions>=4.14.0,<5\"   \"filelock>=3.15\"   \"numpy<2.1,>=1.24\"   gcsfs==2025.3.0 fsspec==2025.3.0\n",
    "\n",
    "from importlib import metadata as md\n",
    "def _ver(name, mod=None):\n",
    "    try:\n",
    "        return md.version(name)\n",
    "    except Exception:\n",
    "        try:\n",
    "            m = __import__(mod or name)\n",
    "            return getattr(m, \"__version__\", \"unknown\")\n",
    "        except Exception:\n",
    "            return \"not installed\"\n",
    "print(\n",
    "  \"[compat]\",\n",
    "  \"jedi=\", _ver(\"jedi\"),\n",
    "  \"typing_extensions=\", _ver(\"typing_extensions\",\"typing_extensions\"),\n",
    "  \"filelock=\", _ver(\"filelock\"),\n",
    "  \"numpy=\", _ver(\"numpy\"),\n",
    "  \"gcsfs=\", _ver(\"gcsfs\"),\n",
    "  \"fsspec=\", _ver(\"fsspec\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9813e96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9813e96",
    "outputId": "d5678850-8e59-40a3-846f-886c22fc5c10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SA key: /content/Secrets/sa.json\n",
      "[mount] pik_source_bucket -> /content/src_gcs: True\n",
      "[mount] pik-artifacts-dev -> /content/artifacts: True\n",
      "mount src= True  mount artifacts= True\n"
     ]
    }
   ],
   "source": [
    "#@title Mount GCS buckets\n",
    "require_start()\n",
    "\n",
    "# Robust mount with verbose logs and fallback info\n",
    "import os, glob, subprocess, pathlib, textwrap\n",
    "pathlib.Path('/content/src_gcs').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('/content/artifacts').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('/content/gcsfuse_tmp').mkdir(parents=True, exist_ok=True)\n",
    "key = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', '')\n",
    "if (not key) and 'KEY' in globals(): key = KEY\n",
    "if key and not os.path.isabs(key): key = os.path.join('/content', key)\n",
    "if not (key and os.path.exists(key)):\n",
    "  matches = glob.glob('/content/Secrets/*.json')\n",
    "  if matches:\n",
    "    key = matches[0]; os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key\n",
    "  else:\n",
    "    raise SystemExit('Service account key not found — upload to /content/Secrets/*.json')\n",
    "print('Using SA key:', key)\n",
    "def mount(bucket, mnt):\n",
    "  log=f'/content/gcsfuse_{bucket}.log'.replace('/', '_')\n",
    "  cmd=['gcsfuse','--implicit-dirs','--key-file', key,'--temp-dir','/content/gcsfuse_tmp','--log-file',log, bucket, mnt]\n",
    "  res=subprocess.run(cmd, capture_output=True, text=True)\n",
    "  ok=(res.returncode==0)\n",
    "  print(f'[mount] {bucket} -> {mnt}:', ok)\n",
    "  if not ok:\n",
    "    print('[mount] stdout:\\n' + res.stdout)\n",
    "    print('[mount] stderr:\\n' + res.stderr)\n",
    "    try:\n",
    "      tail=subprocess.run(['bash','-lc', f'tail -n 80 {log}'], capture_output=True, text=True)\n",
    "      if tail.stdout: print('[mount] log tail:\\n' + tail.stdout)\n",
    "    except Exception: pass\n",
    "  return ok\n",
    "subprocess.run(['fusermount','-u','/content/src_gcs'], check=False)\n",
    "subprocess.run(['fusermount','-u','/content/artifacts'], check=False)\n",
    "# Quick bucket existence check\n",
    "for b in ('pik_source_bucket','pik-artifacts-dev'):\n",
    "  subprocess.run(['bash','-lc', f'gsutil ls -b gs://{b} || true'], check=False)\n",
    "ok1 = mount('pik_source_bucket','/content/src_gcs')\n",
    "ok2 = mount('pik-artifacts-dev','/content/artifacts')\n",
    "print('mount src=', ok1, ' mount artifacts=', ok2)\n",
    "if not (ok1 and ok2):\n",
    "  print(textwrap.dedent('''\n",
    "    [hint] If mount keeps failing:\n",
    "      - Check SA has Storage Object Admin on both buckets\n",
    "      - Try fallback: copy files with gsutil (already used elsewhere in the notebook)\n",
    "      - Ensure bucket names are correct and exist (see checks above)\n",
    "    '''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6309994d",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6309994d",
    "outputId": "714471ea-9c51-4419-9b8d-f1142a585054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 9.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "pytensor 2.31.7 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytensor 2.31.7 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  DEPRECATION: Building 'segment_anything' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'segment_anything'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  DEPRECATION: Building 'iopath' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'iopath'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCloning into '/content/GroundingDINO'...\n",
      "remote: Enumerating objects: 78, done.\u001b[K\n",
      "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
      "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
      "remote: Total 78 (delta 4), reused 76 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (78/78), 10.65 MiB | 34.28 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  ninja-build\n",
      "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
      "Need to get 111 kB of archives.\n",
      "After this operation, 358 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ninja-build amd64 1.10.1-1 [111 kB]\n",
      "Fetched 111 kB in 0s (255 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package ninja-build.\n",
      "(Reading database ... 126411 files and directories currently installed.)\n",
      "Preparing to unpack .../ninja-build_1.10.1-1_amd64.deb ...\n",
      "Unpacking ninja-build (1.10.1-1) ...\n",
      "Setting up ninja-build (1.10.1-1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  DEPRECATION: Building 'groundingdino' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'groundingdino'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for groundingdino (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "GroundingDINO pip install OK\n",
      "GroundedDINO import OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    }
   ],
   "source": [
    "#@title Install Torch + SAM/SAM2 + GroundedDINO\n",
    "require_start()\n",
    "\n",
    "# 2) Torch + детекторы — надёжная установка (без сборки wheel для GroundedDINO)\n",
    "!pip -q install --upgrade pip setuptools wheel\n",
    "# Ensure IPython+jedi and core libs for resolver\n",
    "!pip -q install -U jedi>=0.16 typing_extensions>=4.14.0 filelock>=3.15\n",
    "!pip -q install --upgrade --force-reinstall torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip -q install 'numpy<2.1,>=1.24'\n",
    "!pip -q install shapely timm opencv-python pycocotools addict yacs requests pillow\n",
    "!pip -q install huggingface_hub\n",
    "# xformers optional; skipped by default due to wheel/torch version coupling\n",
    "# SAM\n",
    "!pip -q install git+https://github.com/facebookresearch/segment-anything.git\n",
    "# SAM2\n",
    "!pip -q install git+https://github.com/facebookresearch/segment-anything-2.git\n",
    "# GroundedDINO из исходников (подключим через sys.path)\n",
    "!rm -rf /content/GroundingDINO\n",
    "!git clone --depth 1 https://github.com/IDEA-Research/GroundingDINO.git /content/GroundingDINO\n",
    "!pip -q install -r /content/GroundingDINO/requirements.txt\n",
    "# Build C++/CUDA ops for GroundingDINO (ms_deform_attn _C)\n",
    "!sudo apt-get -q install -y ninja-build\n",
    "!pip -q install \"git+https://github.com/IDEA-Research/GroundingDINO.git\" || echo 'pip install from git failed; will import from source'\n",
    "import sys, os, glob, subprocess\n",
    "try:\n",
    "  import groundingdino\n",
    "  print('GroundingDINO pip install OK')\n",
    "except Exception as e:\n",
    "  print('GroundingDINO pip install failed; building from source:', e)\n",
    "  cands = [p for p in glob.glob('/content/GroundingDINO/**/setup.py', recursive=True) if ('ms_deform' in p) or ('ops' in p)]\n",
    "  for sp in cands:\n",
    "    d=os.path.dirname(sp); print('Building ext in', d)\n",
    "    subprocess.run([sys.executable, 'setup.py', 'build_ext', '--inplace'], cwd=d, check=False)\n",
    "  if '/content/GroundingDINO' not in sys.path: sys.path.append('/content/GroundingDINO')\n",
    "  import groundingdino\n",
    "  print('GroundingDINO import OK (source fallback)')\n",
    "# Build C++/CUDA ops for GroundingDINO (ms_deform_attn _C)\n",
    "import sys\n",
    "if '/content/GroundingDINO' not in sys.path: sys.path.append('/content/GroundingDINO')\n",
    "from groundingdino.util.inference import Model\n",
    "print('GroundedDINO import OK')\n",
    "# Compatibility pins (final)\n",
    "!pip -q uninstall -y xformers || true\n",
    "!pip -q install -U 'numpy<2.1,>=1.24' typing_extensions>=4.14.0 filelock>=3.15\n",
    "!pip -q install -U gcsfs==2025.3.0 fsspec==2025.3.0\n",
    "import importlib, pkgutil;\n",
    "print('[versions]',\n",
    "      'torch', __import__('torch').__version__,\n",
    "      'numpy', __import__('numpy').__version__,\n",
    "      'typing_extensions', __import__('typing_extensions').__version__,\n",
    "      'filelock', __import__('filelock').__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3faa9ae",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3faa9ae",
    "outputId": "1043b25f-3019-43d7-d5c8-a34fd782f021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.5.1+cu124 cuda: 12.4 available: True\n",
      "torchvision: 0.20.1+cu121\n",
      "[OK] torchvision.ops on CUDA\n",
      "GroundingDINO _C present: True\n",
      "[SELF-CHECK] req= auto  cuda_avail= True  tv_ops_ok= True  dino_ops_ok= True\n"
     ]
    }
   ],
   "source": [
    "#@title CUDA and C++ ops self-check\n",
    "import warnings, importlib, torch, torchvision, os\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "req = (DEVICE.lower() if 'DEVICE' in globals() else 'auto')\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "print('torch:', torch.__version__, 'cuda:', torch.version.cuda, 'available:', cuda_avail)\n",
    "print('torchvision:', torchvision.__version__)\n",
    "tv_ops_ok=False\n",
    "if cuda_avail:\n",
    "  try:\n",
    "    x=torch.rand(256,4,device='cuda'); y=torch.rand(256,4,device='cuda')\n",
    "    from torchvision.ops import box_iou\n",
    "    _=box_iou(x,y); tv_ops_ok=True; print('[OK] torchvision.ops on CUDA')\n",
    "  except Exception as e:\n",
    "    print('[WARN] torchvision.ops CUDA failed:', e)\n",
    "try:\n",
    "  m=importlib.import_module('groundingdino.models.GroundingDINO.ms_deform_attn')\n",
    "  dino_ops_ok=bool(getattr(m,'_C', None))\n",
    "  print('GroundingDINO _C present:', dino_ops_ok)\n",
    "except Exception as e:\n",
    "  dino_ops_ok=False; print('[WARN] GroundingDINO C++ ops import failed:', e)\n",
    "# Hard assertions when DEVICE='cuda'\n",
    "if req=='cuda':\n",
    "  assert cuda_avail, 'CUDA requested but not available'\n",
    "  assert tv_ops_ok, 'torchvision CUDA ops unavailable'\n",
    "  assert dino_ops_ok, 'GroundingDINO C++ ops (_C) not built'\n",
    "print('[SELF-CHECK] req=', req, ' cuda_avail=', cuda_avail, ' tv_ops_ok=', tv_ops_ok, ' dino_ops_ok=', dino_ops_ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d75b85",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "98eb1a1ef37c458e93f0e30d2fd9ba49",
      "0b05065b9c514c3d93881cada39143a9",
      "ce420c71e3ce434095e4b3d9b1b5c6a3",
      "de3ab9f78a094c498cc2e09ad7e6d322",
      "0e373a8a99e5408ab5688e19f0b288bf",
      "88452d3814a44ca79ec90918ae6ac130",
      "6538d66c0ad64df1be949bd94112c73f",
      "1ee8fb8020834542b363ac28e68115ee",
      "6bcd5b1057954e009e56e778ae1d2892",
      "f78a857fcd0c455283fcb85870c8bf8e",
      "528cd4ba918b45028946c166cee88fb6"
     ]
    },
    "id": "79d75b85",
    "outputId": "dd16347c-632d-47b2-f090-19fb64878fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GroundingDINO weights (robust)...\n",
      "[DINO] using GCS mirror\n",
      "Downloading SAM ViT-H weights (robust)...\n",
      "[SAM] using GCS mirror\n",
      "Downloading SAM2 Hiera Large weights (robust)...\n",
      "[warn] copy SAM2 from GCS mirror failed: [Errno 2] No such file or directory: '/content/models/sam2/sam2_hiera_large.pt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb1a1ef37c458e93f0e30d2fd9ba49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sam2_hiera_large.pt:   0%|          | 0.00/898M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAM2] using HF Hub\n",
      "[DINO] size= 693997677\n",
      "GROUNDING_MODEL = /content/models/groundingdino/groundingdino_swint_ogc.pth\n",
      "[SAM2] size= 897952466\n",
      "[SAM] size= 2564550879\n",
      "SAM_MODEL       = /content/models/sam/sam_vit_h_4b8939.pth\n"
     ]
    }
   ],
   "source": [
    "#@title Download/Resolve Model Weights\n",
    "#@title Download/Resolve Model Weights\n",
    "# (Optional) Download model weights if not present\n",
    "import os, pathlib, shutil, subprocess\n",
    "from typing import Optional\n",
    "pathlib.Path('/content/models/groundingdino').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('/content/models/sam').mkdir(parents=True, exist_ok=True)\n",
    "GROUNDING_MODEL = '/content/models/groundingdino/groundingdino_swint_ogc.pth'\n",
    "SAM_MODEL = '/content/models/sam/sam_vit_h_4b8939.pth'\n",
    "GROUNDING_URL = 'https://github.com/IDEA-Research/GroundingDINO/releases/download/0.1.0/groundingdino_swint_ogc.pth'\n",
    "SAM_URL = 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'\n",
    "SAM2_MODEL = '/content/models/sam2/sam2_hiera_large.pt'\n",
    "SAM2_URL = 'https://huggingface.co/facebook/sam2-hiera-large/resolve/main/sam2_hiera_large.pt'\n",
    "# Try to read HF token from Colab Keys or env\n",
    "HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
    "try:\n",
    "  from google.colab import userdata as _ud\n",
    "  HF_TOKEN = _ud.get('HF_TOKEN') or HF_TOKEN\n",
    "except Exception:\n",
    "  pass\n",
    "def _file_ok(p: str, min_size: int) -> bool:\n",
    "  try:\n",
    "    return os.path.exists(p) and os.path.getsize(p) >= min_size\n",
    "  except Exception:\n",
    "    return False\n",
    "def _try_torch_load(p: str) -> bool:\n",
    "  try:\n",
    "    import torch\n",
    "    torch.load(p, map_location='cpu')\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    print('[warn] torch.load failed:', e)\n",
    "    return False\n",
    "def _hf_download(repo_id: str, filename: str, dest: str) -> bool:\n",
    "  try:\n",
    "    from huggingface_hub import hf_hub_download, login\n",
    "    if HF_TOKEN:\n",
    "      try:\n",
    "        login(token=HF_TOKEN)\n",
    "      except Exception as e:\n",
    "        print('[warn] HF login failed:', e)\n",
    "    ckpt = hf_hub_download(repo_id=repo_id, filename=filename, local_dir=os.path.dirname(dest), local_dir_use_symlinks=False, token=HF_TOKEN or None)\n",
    "    if ckpt != dest:\n",
    "      shutil.copy2(ckpt, dest)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    print('[warn] HF download failed:', e)\n",
    "    return False\n",
    "def _curl(url: str, dest: str, min_size: int) -> bool:\n",
    "  cmd = f\"curl -L --fail --retry 5 --retry-all-errors -o '{dest}.tmp' '{url}'\"\n",
    "  rc = subprocess.call(cmd, shell=True)\n",
    "  if rc == 0 and _file_ok(dest + '.tmp', min_size):\n",
    "    shutil.move(dest + '.tmp', dest)\n",
    "    return True\n",
    "  else:\n",
    "    print('[warn] curl download insufficient or failed:', rc)\n",
    "    try:\n",
    "      os.remove(dest + '.tmp')\n",
    "    except Exception:\n",
    "      pass\n",
    "    return False\n",
    "# GroundedDINO (expect ~0.9GB)\n",
    "MIN_DINO = 600_000_000\n",
    "need_dino = (not _file_ok(GROUNDING_MODEL, MIN_DINO)) or (not _try_torch_load(GROUNDING_MODEL))\n",
    "if need_dino:\n",
    "  print('Downloading GroundingDINO weights (robust)...')\n",
    "  try:\n",
    "    os.remove(GROUNDING_MODEL)\n",
    "  except Exception:\n",
    "    pass\n",
    "  # 1) Try GCS mirror if mounted\n",
    "  gcs_mirror = '/content/artifacts/models/groundingdino/groundingdino_swint_ogc.pth'\n",
    "  ok = _file_ok(gcs_mirror, MIN_DINO)\n",
    "  if ok:\n",
    "    try:\n",
    "      shutil.copy2(gcs_mirror, GROUNDING_MODEL)\n",
    "      print('[DINO] using GCS mirror')\n",
    "      ok = True\n",
    "    except Exception as e:\n",
    "      print('[warn] copy from GCS mirror failed:', e); ok = False\n",
    "  # 2) Try HF Hub (public repo)\n",
    "  if (not ok):\n",
    "    ok = _hf_download('ShilongLiu/GroundingDINO', 'groundingdino_swint_ogc.pth', GROUNDING_MODEL)\n",
    "    if ok: print('[DINO] using HF Hub')\n",
    "  # 3) Try GitHub release via curl\n",
    "  if (not ok) or (not _file_ok(GROUNDING_MODEL, MIN_DINO)):\n",
    "    ok = _curl(GROUNDING_URL, GROUNDING_MODEL, MIN_DINO)\n",
    "    if ok: print('[DINO] using curl URL')\n",
    "  # 4) Try gsutil from bucket path if accessible\n",
    "  if (not ok) or (not _file_ok(GROUNDING_MODEL, MIN_DINO)):\n",
    "    try:\n",
    "      rc = subprocess.call(f\"gsutil cp gs://pik-artifacts-dev/models/groundingdino/groundingdino_swint_ogc.pth '{GROUNDING_MODEL}'\", shell=True)\n",
    "      ok = (rc == 0) and _file_ok(GROUNDING_MODEL, MIN_DINO)\n",
    "      if ok: print('[DINO] using gsutil mirror')\n",
    "    except Exception as e:\n",
    "      print('[warn] gsutil mirror copy failed:', e)\n",
    "  if (not ok) or (not _file_ok(GROUNDING_MODEL, MIN_DINO)) or (not _try_torch_load(GROUNDING_MODEL)):\n",
    "    raise SystemExit('Failed to fetch a valid GroundingDINO checkpoint')\n",
    "# SAM (ViT-H is large; check size only)\n",
    "MIN_SAM = 1_000_000_000\n",
    "if not _file_ok(SAM_MODEL, MIN_SAM):\n",
    "  print('Downloading SAM ViT-H weights (robust)...')\n",
    "  # 1) Try GCS mirror if mounted\n",
    "  sam_gcs_mirror = '/content/artifacts/models/sam/sam_vit_h_4b8939.pth'\n",
    "  ok = _file_ok(sam_gcs_mirror, MIN_SAM)\n",
    "  if ok:\n",
    "    try:\n",
    "      shutil.copy2(sam_gcs_mirror, SAM_MODEL)\n",
    "      print('[SAM] using GCS mirror')\n",
    "      ok = True\n",
    "    except Exception as e:\n",
    "      print('[warn] copy SAM from GCS mirror failed:', e); ok = False\n",
    "  # 2) Try HF Hub\n",
    "  if (not ok):\n",
    "    ok = _hf_download('facebook/sam', 'sam_vit_h_4b8939.pth', SAM_MODEL)\n",
    "    if ok: print('[SAM] using HF Hub')\n",
    "  # 3) Try official URL via curl\n",
    "  if (not ok) or (not _file_ok(SAM_MODEL, MIN_SAM)):\n",
    "    ok = _curl(SAM_URL, SAM_MODEL, MIN_SAM)\n",
    "    if ok: print('[SAM] using curl URL')\n",
    "  # 4) Try gsutil mirror from bucket\n",
    "  if (not ok) or (not _file_ok(SAM_MODEL, MIN_SAM)):\n",
    "    try:\n",
    "      rc = subprocess.call(f\"gsutil cp gs://pik-artifacts-dev/models/sam/sam_vit_h_4b8939.pth '{SAM_MODEL}'\", shell=True)\n",
    "      ok = (rc == 0) and _file_ok(SAM_MODEL, MIN_SAM)\n",
    "      if ok: print('[SAM] using gsutil mirror')\n",
    "    except Exception as e:\n",
    "      print('[warn] gsutil SAM mirror copy failed:', e)\n",
    "  if (not ok) or (not _file_ok(SAM_MODEL, MIN_SAM)):\n",
    "    raise SystemExit('Failed to fetch SAM ViT-H checkpoint')\n",
    "# SAM2 (Hiera Large)\n",
    "MIN_SAM2 = 700_000_000\n",
    "if not _file_ok(SAM2_MODEL, MIN_SAM2):\n",
    "  print('Downloading SAM2 Hiera Large weights (robust)...')\n",
    "  # 1) Try GCS mirror if mounted\n",
    "  sam2_gcs_mirror = '/content/artifacts/models/sam2/sam2_hiera_large.pt'\n",
    "  ok = _file_ok(sam2_gcs_mirror, MIN_SAM2)\n",
    "  if ok:\n",
    "    try:\n",
    "      shutil.copy2(sam2_gcs_mirror, SAM2_MODEL)\n",
    "      print('[SAM2] using GCS mirror')\n",
    "      ok = True\n",
    "    except Exception as e:\n",
    "      print('[warn] copy SAM2 from GCS mirror failed:', e); ok = False\n",
    "  # 2) Try HF Hub\n",
    "  if (not ok):\n",
    "    ok = _hf_download('facebook/sam2-hiera-large', 'sam2_hiera_large.pt', SAM2_MODEL)\n",
    "    if ok: print('[SAM2] using HF Hub')\n",
    "  # 3) Try direct URL via curl\n",
    "  if (not ok) or (not _file_ok(SAM2_MODEL, MIN_SAM2)):\n",
    "    ok = _curl(SAM2_URL, SAM2_MODEL, MIN_SAM2)\n",
    "    if ok: print('[SAM2] using curl URL')\n",
    "  # 4) Try gsutil mirror from bucket\n",
    "  if (not ok) or (not _file_ok(SAM2_MODEL, MIN_SAM2)):\n",
    "    try:\n",
    "      rc = subprocess.call(f\"gsutil cp gs://pik-artifacts-dev/models/sam2/sam2_hiera_large.pt '{SAM2_MODEL}'\", shell=True)\n",
    "      ok = (rc == 0) and _file_ok(SAM2_MODEL, MIN_SAM2)\n",
    "      if ok: print('[SAM2] using gsutil mirror')\n",
    "    except Exception as e:\n",
    "      print('[warn] gsutil SAM2 mirror copy failed:', e)\n",
    "  if (not ok) or (not _file_ok(SAM2_MODEL, MIN_SAM2)):\n",
    "    raise SystemExit('Failed to fetch SAM2 Hiera Large checkpoint')\n",
    "import os as _os; print('[DINO] size=', _os.path.getsize(GROUNDING_MODEL)); print('GROUNDING_MODEL =', GROUNDING_MODEL)\n",
    "import os as _os; print('[SAM2] size=', _os.path.getsize(SAM2_MODEL))\n",
    "import os as _os; print('[SAM] size=', _os.path.getsize(SAM_MODEL)); print('SAM_MODEL       =', SAM_MODEL)\n",
    "\n",
    "# Log weights info if logging enabled\n",
    "try:\n",
    "  import os as _os\n",
    "  WEIGHTS_INFO = {\n",
    "    'groundingdino': {'path': GROUNDING_MODEL, 'size': _os.path.getsize(GROUNDING_MODEL)},\n",
    "    'sam': {'path': SAM_MODEL, 'size': _os.path.getsize(SAM_MODEL)},\n",
    "    'sam2': ({'path': SAM2_MODEL, 'size': _os.path.getsize(SAM2_MODEL)} if _os.path.exists(SAM2_MODEL) else None),\n",
    "  }\n",
    "  if 'log_json' in globals(): log_json('weights.json', WEIGHTS_INFO)\n",
    "except Exception as e:\n",
    "  print('[LOG] weights info not recorded:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56041eff",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56041eff",
    "outputId": "15f11a0d-cf54-45a4-9b05-2ba744d1e901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: /content/src_gcs/playbooks/PIK - Expert Guide - Platform IT Architecture - Playbook - v11.pdf Pages: [4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "#@title Selected Parameters (echo)\n",
    "# This cell only echoes current config defined in the top control cell.\n",
    "try:\n",
    "    FRAME_NAMES\n",
    "except NameError:\n",
    "    FRAME_NAMES = []\n",
    "try:\n",
    "    PROMPTS\n",
    "except NameError:\n",
    "    PROMPTS = []\n",
    "print('START_RUN=', START_RUN)\n",
    "print('PDF:', PLAYBOOK_PDF)\n",
    "print('PAGES (empty=ALL):', PAGES)\n",
    "print('Frames:', FRAME_NAMES)\n",
    "print('Prompts:', PROMPTS)\n",
    "print('Device:', DEVICE, 'Use SAM2:', USE_SAM2)\n",
    "print('Report to GCS:', REPORT_TO_GCS, 'Bucket:', GCS_BUCKET, 'Run tag:', RUN_TAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7f44ca",
   "metadata": {
    "cellView": "form",
    "id": "cb7f44ca"
   },
   "outputs": [],
   "source": [
    "#@title Logging helpers (GCS)\n",
    "import os, sys, json, time, platform, socket, subprocess\n",
    "from pathlib import Path\n",
    "RUN_ID = time.strftime('%Y%m%d-%H%M%S') + (('-'+RUN_TAG.strip()) if ('RUN_TAG' in globals() and RUN_TAG.strip()) else '')\n",
    "LOCAL_LOG_ROOT = '/content/artifacts/colab_runs' if os.path.exists('/content/artifacts') else '/content/colab_runs'\n",
    "LOG_DIR = Path(LOCAL_LOG_ROOT)/RUN_ID\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def log_json(name, obj):\n",
    "  p = LOG_DIR/name if isinstance(name, Path) else LOG_DIR/str(name)\n",
    "  p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "def log_kv(key, val):\n",
    "  data = {}\n",
    "  p = LOG_DIR/'run.json'\n",
    "  if p.exists():\n",
    "    try: data = json.loads(p.read_text(encoding='utf-8'))\n",
    "    except Exception: data = {}\n",
    "  data[key]=val\n",
    "  p.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "env = { 'python': sys.version, 'platform': platform.platform(), 'hostname': socket.gethostname(), 'device_req': (DEVICE if 'DEVICE' in globals() else 'auto') }\n",
    "try:\n",
    "  import torch\n",
    "  env.update({'torch': torch.__version__, 'cuda': getattr(torch.version,'cuda',None), 'cuda_available': torch.cuda.is_available()})\n",
    "except Exception: pass\n",
    "log_json('env.json', env)\n",
    "def upload_logs():\n",
    "  if not ('REPORT_TO_GCS' in globals() and REPORT_TO_GCS):\n",
    "    print('[LOG] REPORT_TO_GCS disabled'); return\n",
    "  bucket = (GCS_BUCKET if 'GCS_BUCKET' in globals() else 'pik-artifacts-dev')\n",
    "  prefix = f'colab_runs/{RUN_ID}'\n",
    "  try:\n",
    "    from google.cloud import storage\n",
    "    client=storage.Client()\n",
    "    b=client.bucket(bucket)\n",
    "    for p in LOG_DIR.rglob('*'):\n",
    "      if p.is_file():\n",
    "        rel=str(p.relative_to(LOG_DIR)).replace('\\\\','/')\n",
    "        blob=b.blob(f'{prefix}/{rel}')\n",
    "        ctype='application/json' if p.suffix.lower()=='.json' else 'text/plain; charset=utf-8'\n",
    "        blob.content_type=ctype\n",
    "        blob.upload_from_filename(str(p))\n",
    "    print(f'[LOG] uploaded to gs://{bucket}/{prefix}')\n",
    "    return\n",
    "  except Exception as e:\n",
    "    print('[LOG] storage client failed, fallback to gsutil:', e)\n",
    "  # gsutil fallback\n",
    "  cmd=f\"gsutil -m cp -r '{LOG_DIR}' gs://{bucket}/colab_runs/\"\n",
    "  subprocess.run(['bash','-lc', cmd], check=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d698d22",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d698d22",
    "outputId": "786a3706-8437-46f3-8f30-d647ce4b139d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering 4\n",
      "Rendering 5\n",
      "Rendering 6\n",
      "Rendering 7\n",
      "Rendering 8\n",
      "Rendering 9\n",
      "Rendering 10\n",
      "Rendering 11\n",
      "total 4044\n",
      "drwxr-xr-x 2 root root   4096 Sep 16 04:32 .\n",
      "drwxr-xr-x 1 root root   4096 Sep 16 04:32 ..\n",
      "-rw-r--r-- 1 root root 512200 Sep 16 04:32 page-10.png\n",
      "-rw-r--r-- 1 root root 512200 Sep 16 04:32 page-11.png\n",
      "-rw-r--r-- 1 root root 512200 Sep 16 04:32 page-4.png\n",
      "-rw-r--r-- 1 root root 512200 Sep 16 04:32 page-5.png\n",
      "-rw-r--r-- 1 root root 512200 Sep 16 04:32 page-6.png\n",
      "-rw-r--r-- 1 root root 512200 Sep 16 04:32 page-7.png\n",
      "-rw-r--r-- 1 root root 512200 Sep 16 04:32 page-8.png\n"
     ]
    }
   ],
   "source": [
    "#@title Render Pages to PNG\n",
    "require_start()\n",
    "\n",
    "# Render selected pages to PNG (robust: checks poppler + PDF presence; falls back to gsutil cp)\n",
    "import os, shutil, pathlib, subprocess\n",
    "from subprocess import check_call\n",
    "pathlib.Path(OUT_PAGES_DIR).mkdir(parents=True, exist_ok=True)\n",
    "# Ensure pdftoppm exists\n",
    "if not shutil.which('pdftoppm'):\n",
    "  print('Installing poppler-utils (pdftoppm)...')\n",
    "  check_call(['bash','-lc','sudo apt-get -q update && sudo apt-get -q install -y poppler-utils'])\n",
    "# Ensure source PDF exists; if not, copy from GCS via gsutil\n",
    "src = PLAYBOOK_PDF\n",
    "if not os.path.exists(src):\n",
    "  print('PDF not found at', src, '; copying from GCS...')\n",
    "  check_call(['bash','-lc','gsutil -m cp \"gs://pik_source_bucket/playbooks/PIK - Expert Guide - Platform IT Architecture - Playbook - v11.pdf\" /content/Playbook.pdf'])\n",
    "  src = '/content/Playbook.pdf'\n",
    "# Render pages\n",
    "# If PAGES is empty, compute all pages via pdfinfo\n",
    "def _detect_all_pages(pdf_path):\n",
    "    import subprocess, re\n",
    "    try:\n",
    "        out = subprocess.check_output(['pdfinfo', pdf_path], text=True)\n",
    "        m = re.search(r'^Pages:\\\\s+(\\\\d+)', out, re.M|re.I)\n",
    "        return int(m.group(1)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "pages_selected = list(PAGES) if isinstance(PAGES, list) else []\n",
    "if not pages_selected:\n",
    "    n = _detect_all_pages(src)\n",
    "    if n:\n",
    "        pages_selected = list(range(1, n+1))\n",
    "    else:\n",
    "        raise SystemExit('Cannot determine page count and PAGES is empty')\n",
    "print('Pages to render:', pages_selected[:20], ('...' if len(pages_selected)>20 else ''))\n",
    "\n",
    "for p in pages_selected:\n",
    "  print('Rendering', p)\n",
    "  check_call(['pdftoppm','-png','-singlefile','-r','150', src, f'{OUT_PAGES_DIR}/page-{p}'])\n",
    "!ls -la /content/pages | head -n 10\n",
    "def ensure_dir(d):\n",
    "  pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4be7c64",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d6ca8dc7927c487a914b86084ea706e4",
      "ff07e9e455824969a0911e955d48559e",
      "dcf193fb13594a69b5ee64635ff1ff04",
      "a39da53686424c53a792ab33f9fcc913",
      "22a40fd51cda47a79a0610e567040c8c",
      "7d1e18c2c40f4596874f9d46db155f6c",
      "83514a0f30c9443793fe168af0a83e80",
      "0f10b2d0fd364ae3bf2dfad2365634d8",
      "80e669baeec04ef1869cc4f52f73f06a",
      "7d39ec5414b54104b5da7beeed346b95",
      "4fcd76095c0744839af03a1b5eb2cf60",
      "1914a199b86e42a48ab21dd63c45df5f",
      "f4764ff00593404ca6d30ff6833e1956",
      "8842d175673043639843d0bae6bc0baa",
      "44766a3055fb4bce8631eb4c78781800",
      "e54558d6c7024afd891b959bf7f63de9",
      "3643427cffa548a3a3f6dfd2f05a88b8",
      "a186a256553e42bf82651e2c43385f0f",
      "03e091558a2a4d98a6e05e59f7575b35",
      "a3f55595a7214067abf2e28ef9237253",
      "15e7959ac5e44994b102b135ad16070d",
      "4eeb9cc426e841bcbfeb3990b209bcf5",
      "13265fe422f54650893e21c4f2e7232f",
      "57f4deac91244e4ba080e968c48940d2",
      "d9aa0a388d374f31954a18850a3ed3a5",
      "58125cc7a9624bf49c735647932e6db1",
      "27b8166283fc4d0baab1f345583b7b01",
      "3782688c468e470099c4dabcada75d21",
      "3d031af008944b8996d59d3305fff493",
      "82b7489aa62f4e7ab5b4f3b01599ca53",
      "b39a639c478d4c8397e4b8ddb8790835",
      "89b425ad863f4f728fac6b810b115e20",
      "8d52f76a29ef47938c988a916d638831",
      "9297be505cfd48309cfe109f9a7cb373",
      "ad7940ade166429f91f00e41ef9eea5f",
      "1d9b11942452476aa34f82be14d14273",
      "a71c2434023443babe12f6265052e07d",
      "824eb5611b4a42a99c669d33c7664f5f",
      "306e64f15c7b4a768fb023ee1ca22438",
      "92e48a19e91c4966b7cde6a31f5ca3b0",
      "6db9b8a73d954bf4b6c735dd2056e98d",
      "04900f313c024335a4848e0646a735ec",
      "b911ea6216d34954984353d3e4fd54a1",
      "12f26ade312e4c90b533f8321461bf0a",
      "c65c9a1f94624ebdb1a85ed656c92467",
      "55e8c6c934a640e7bbb8a22f84380457",
      "9e725bb5b7dc449fa540c6b983647351",
      "29ece67a6a0b4144812bc0a92f2d0000",
      "aeb098bfea3447b592b5b054942199ca",
      "6b18a0e81219461f8ba03b70d7159770",
      "adeda8dcaf8940b5a7da55756e5c24c3",
      "534ead4d03f54197969215cd38304e2a",
      "92d4780fd20a4d2cb3ce4e6a949f390e",
      "28f9d992a46c40fc8a98b2e2b5fe54ff",
      "426649f9c8c34c1b809d420e8c105a31"
     ]
    },
    "id": "c4be7c64",
    "outputId": "68b3080a-f062-4ef1-9e17-304ed80a3067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n",
      "SAM2 init failed, fallback to SAM v1: Cannot find primary config 'sam2_hiera_large'. Check that it's in your config search path.\n",
      "\n",
      "Config search path:\n",
      "\tprovider=hydra, path=pkg://hydra.conf\n",
      "\tprovider=main, path=pkg://sam2\n",
      "\tprovider=schema, path=structured://\n",
      "SAM v1 ready on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ca8dc7927c487a914b86084ea706e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1914a199b86e42a48ab21dd63c45df5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13265fe422f54650893e21c4f2e7232f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9297be505cfd48309cfe109f9a7cb373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65c9a1f94624ebdb1a85ed656c92467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[  34.62677  ,    3.7105103, 1873.7822   , 1057.823    ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.42326838], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[   7.5255127,    6.001831 , 2067.433    , 1390.9086   ],\n",
      "       [   7.2773438,    6.763489 , 2067.4363   , 1398.0618   ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.5261167 , 0.36346784], dtype=float32), class_id=array([0, 0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[   7.981201 ,    3.1629028, 2065.5935   , 1327.5977   ],\n",
      "       [   7.9503174,    3.5457764, 2066.317    , 1328.5822   ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.4177707 , 0.36369413], dtype=float32), class_id=array([0, 0]), tracker_id=None, data={}, metadata={})\n",
      "Output from predict_with_classes: Detections(xyxy=array([[   6.343506,    7.404724, 2068.0762  , 1367.9912  ]],\n",
      "      dtype=float32), mask=None, confidence=array([0.46676695], dtype=float32), class_id=array([0]), tracker_id=None, data={}, metadata={})\n",
      "\n",
      "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
      "both the source and destination. Your crcmod installation isn't using the\n",
      "module's C extension, so checksumming will run very slowly. If this is your\n",
      "first rsync since updating gsutil, this rsync can take significantly longer than\n",
      "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///content/grounded_regions/page-10/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/page-11/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/page-10/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-11/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-6/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/page-4/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/page-6/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-7/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/page-7/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-4/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-5/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/page-8/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/page-5/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-9/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-8/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/page-9/regions/region-1.json [Content-Type=application/json]...\n",
      "- [16/16 files][  5.8 MiB/  5.8 MiB] 100% Done                                  \n",
      "Operation completed over 16 objects/5.8 MiB.                                     \n",
      "\n",
      "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
      "both the source and destination. Your crcmod installation isn't using the\n",
      "module's C extension, so checksumming will run very slowly. If this is your\n",
      "first rsync since updating gsutil, this rsync can take significantly longer than\n",
      "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///content/grounded_frames/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_frames/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvases - v01/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-2.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvases - v01/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-2.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvases - v01/regions/region-2.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_frames/PIK - Platform IT Architecture Canvases - v01/regions/region-2.png [Content-Type=image/png]...\n",
      "- [10/10 files][  1.6 MiB/  1.6 MiB] 100% Done                                  \n",
      "Operation completed over 10 objects/1.6 MiB.                                     \n",
      "gs://pik-artifacts-dev/grounded_regions/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.png\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-2.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-2.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-1.png\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-2.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-2.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/page-10/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/page-10/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/page-10/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/page-10/regions/region-1.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/page-11/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/page-11/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/page-11/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/page-11/regions/region-1.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/page-4/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/page-4/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/page-4/regions/region-1.json\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Боевой режим: GroundedDINO → SAM (5 страниц + 3 фрейма)\n",
    "require_start()\n",
    "\n",
    "import os, json, pathlib, cv2, numpy as np, torch\n",
    "from groundingdino.util.inference import Model\n",
    "# SAM/SAM2 init with fallback and device control\n",
    "_req = (DEVICE.lower() if 'DEVICE' in globals() else 'auto')\n",
    "if _req == 'cuda' and not torch.cuda.is_available():\n",
    "  print('[warn] CUDA requested but not available; using CPU')\n",
    "  device = 'cpu'\n",
    "elif _req == 'cpu':\n",
    "  device = 'cpu'\n",
    "else:\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device:', device)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "sam2_predictor = None\n",
    "if 'USE_SAM2' in globals() and USE_SAM2:\n",
    "  try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "    sam2_model = build_sam2('sam2_hiera_large', SAM2_MODEL, device=device)\n",
    "    sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "    print('SAM2 ready on', device)\n",
    "  except Exception as e:\n",
    "    print('SAM2 init failed, fallback to SAM v1:', e)\n",
    "    sam2_predictor = None\n",
    "if sam2_predictor is None:\n",
    "  from segment_anything import sam_model_registry, SamPredictor\n",
    "  print('SAM v1 ready on', device)\n",
    "CFG_PATH = '/content/GroundingDINO_SwinT_OGC.py'\n",
    "CFG_URL = 'https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "# Попытка скачать конфиг, если его нет\n",
    "import urllib.request, urllib.error\n",
    "def _download(url, path):\n",
    "  try:\n",
    "    urllib.request.urlretrieve(url, path)\n",
    "    return os.path.exists(path) and os.path.getsize(path) > 1000\n",
    "  except Exception:\n",
    "    return False\n",
    "if not os.path.exists(CFG_PATH):\n",
    "  ok = _download(CFG_URL, CFG_PATH)\n",
    "  if not ok:\n",
    "    try:\n",
    "      import groundingdino, os as _os\n",
    "      CFG_PATH = _os.path.join(_os.path.dirname(groundingdino.__file__), 'config', 'GroundingDINO_SwinT_OGC.py')\n",
    "      print('Using package config at', CFG_PATH)\n",
    "    except Exception as e:\n",
    "      raise FileNotFoundError('GroundingDINO config not found and download failed')\n",
    "# Sanity check on DINO checkpoint\n",
    "import torch\n",
    "try:\n",
    "  _ = torch.load(GROUNDING_MODEL, map_location='cpu')\n",
    "except Exception as e:\n",
    "  raise RuntimeError(f'GroundedDINO checkpoint invalid: {e}')\n",
    "gd_model = Model(model_config_path=CFG_PATH, model_checkpoint_path=GROUNDING_MODEL, device=device)\n",
    "def save_region(rdir, idx, img, xyxy):\n",
    "  x0,y0,x1,y1 = map(int, xyxy)\n",
    "  x0,y0 = max(0,x0), max(0,y0)\n",
    "  crop = img[y0:y1, x0:x1] if y1>y0 and x1>x0 else img\n",
    "  ok, buf = cv2.imencode('.png', crop)\n",
    "  if ok: (rdir/f'region-{idx}.png').write_bytes(buf.tobytes())\n",
    "  obj = { 'bbox': {'x':int(x0),'y':int(y0),'w':int(x1-x0),'h':int(y1-y0)}, 'text':'', 'image_b64':'' }\n",
    "  (rdir/f'region-{idx}.json').write_text(json.dumps(obj, ensure_ascii=False), encoding='utf-8')\n",
    "\n",
    "def ensure_dir(d):\n",
    "    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def detect_one(image_path, out_root):\n",
    "  img = cv2.imread(image_path); assert img is not None, image_path\n",
    "  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  H,W = img_rgb.shape[:2]\n",
    "  output = gd_model.predict_with_classes(image=img_rgb, classes=PROMPTS, box_threshold=BOX_THRESHOLD, text_threshold=TEXT_THRESHOLD)\n",
    "  print(f\"Output from predict_with_classes: {output}\") # Debugging line\n",
    "  boxes = output.xyxy # Access boxes from the Detections object\n",
    "  logits = output.confidence # Access confidence scores from the Detections object\n",
    "  phrases = [PROMPTS[class_id] for class_id in output.class_id] # Infer phrases from class_id and PROMPTS\n",
    "  bxs = []\n",
    "  for b in boxes:\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    if b.max()<=1.01: x0,y0,x1,y1 = b[0]*W, b[1]*H, b[2]*W, b[3]*H\n",
    "    else: x0,y0,x1,y1 = b\n",
    "    bxs.append([x0,y0,x1,y1])\n",
    "  out = os.path.join(out_root, pathlib.Path(image_path).stem, 'regions'); ensure_dir(out)\n",
    "  for i,xyxy in enumerate(bxs[:TOPK], start=1): save_region(pathlib.Path(out), i, img, xyxy)\n",
    "# Страницы\n",
    "ensure_dir(DETECT_OUT)\n",
    "for p in PAGES:\n",
    "  detect_one(f'/content/pages/page-{p}.png', DETECT_OUT)\n",
    "# Фреймы\n",
    "ensure_dir('/content/grounded_frames') # Ensure directory exists\n",
    "for name in FRAME_NAMES:\n",
    "  f = f'/content/src_gcs/frames/{name}'\n",
    "  if os.path.exists(f): detect_one(f, '/content/grounded_frames')\n",
    "# Выгрузка\n",
    "!gsutil -m rsync -r /content/grounded_regions gs://pik-artifacts-dev/grounded_regions/\n",
    "!gsutil -m rsync -r /content/grounded_frames gs://pik-artifacts-dev/grounded_regions/\n",
    "!gsutil ls -r gs://pik-artifacts-dev/grounded_regions | head -n 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed3538c6",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed3538c6",
    "outputId": "ba9338e0-b8c4-4415-974b-d442ddd5877c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
      "both the source and destination. Your crcmod installation isn't using the\n",
      "module's C extension, so checksumming will run very slowly. If this is your\n",
      "first rsync since updating gsutil, this rsync can take significantly longer than\n",
      "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-10/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-11/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-4/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-42/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-45/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-5/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-6/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-7/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-8/\n",
      "gs://pik-artifacts-dev/grounded_regions/page-9/\n",
      "[LOG] uploaded to gs://pik-artifacts-dev/colab_runs/20250916-043238\n"
     ]
    }
   ],
   "source": [
    "#@title Upload Regions to GCS\n",
    "require_start()\n",
    "\n",
    "require_start()\n",
    "\n",
    "# Upload regions to pik-artifacts-dev\n",
    "!gsutil -m rsync -r /content/grounded_regions gs://pik-artifacts-dev/grounded_regions/\n",
    "!gsutil ls gs://pik-artifacts-dev/grounded_regions/ | head -n 20\n",
    "\n",
    "# Upload logs to GCS if enabled\n",
    "try:\n",
    "  if 'upload_logs' in globals(): upload_logs()\n",
    "except Exception as e:\n",
    "  print('[LOG] upload skipped:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VJlE0rWGvJRa",
   "metadata": {
    "id": "VJlE0rWGvJRa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Upload Cell Logs to GCS\n",
    "require_start()\n",
    "from pathlib import Path\n",
    "p = Path(LOG_DIR) / 'cells.jsonl'\n",
    "if not p.exists():\n",
    "  print('[log] cells.jsonl not found at', p)\n",
    "else:\n",
    "  bucket = GCS_BUCKET if 'GCS_BUCKET' in globals() else 'pik-artifacts-dev'\n",
    "  prefix = f'colab_runs/{RUN_ID}' if 'RUN_ID' in globals() else 'colab_runs/manual'\n",
    "  try:\n",
    "    from google.cloud import storage\n",
    "    client = storage.Client()\n",
    "    b = client.bucket(bucket)\n",
    "    blob = b.blob(f'{prefix}/cells.jsonl')\n",
    "    blob.content_type = 'application/json'\n",
    "    blob.upload_from_filename(str(p))\n",
    "    print('[log] uploaded to', f'gs://{bucket}/{prefix}/cells.jsonl')\n",
    "  except Exception as e:\n",
    "    import subprocess\n",
    "    print('[log] storage client failed, fallback to gsutil:', e)\n",
    "    cmd = f\"gsutil cp '{p}' gs://{bucket}/{prefix}/cells.jsonl\"\n",
    "    subprocess.run(['bash','-lc', cmd], check=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}